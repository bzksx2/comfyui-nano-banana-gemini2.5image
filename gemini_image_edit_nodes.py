"""
Gemini å›¾åƒç¼–è¾‘èŠ‚ç‚¹
æ”¯æŒå•å›¾å’Œå¤šå›¾è¾“å…¥ï¼Œè‡ªåŠ¨å¤„ç†æ‰¹æ¬¡æ•°æ®
"""

import torch
import numpy as np
from PIL import Image
import io
import base64
import requests
import json
import time
import random
from typing import Optional, Tuple, Dict, Any, List

try:
    from .tensor_utils import tensor_to_pil, pil_to_tensor, batch_tensor_to_pil_list, get_tensor_info
    from .utils import (
        image_to_base64, base64_to_image,
        validate_api_key, format_error_message, resize_image_for_api
    )
    from .config import DEFAULT_CONFIG
except ImportError:
    from .tensor_utils import tensor_to_pil, pil_to_tensor, batch_tensor_to_pil_list, get_tensor_info
    # Fallback utility functions - å¦‚æœæ— æ³•å¯¼å…¥ï¼Œä½¿ç”¨å†…ç½®ç‰ˆæœ¬
    pass
    
    def image_to_base64(image, format='JPEG'):
        buffer = io.BytesIO()
        if format.upper() == 'JPEG' and image.mode in ('RGBA', 'LA', 'P'):
            background = Image.new('RGB', image.size, (255, 255, 255))
            if image.mode == 'P':
                image = image.convert('RGBA')
            background.paste(image, mask=image.split()[-1] if image.mode in ('RGBA', 'LA') else None)
            image = background
        image.save(buffer, format=format, quality=95)
        return base64.b64encode(buffer.getvalue()).decode('utf-8')
    
    def validate_api_key(api_key):
        return api_key and len(api_key.strip()) > 10
    
    def format_error_message(error):
        return str(error)
    
    DEFAULT_CONFIG = {"timeout": 120, "max_retries": 3}


def smart_retry_delay(attempt, error_code=None):
    """æ™ºèƒ½é‡è¯•å»¶è¿Ÿ"""
    base_delay = 2 ** attempt
    
    if error_code == 429:
        rate_limit_delay = 60 + random.uniform(10, 30)
        return max(base_delay, rate_limit_delay)
    elif error_code in [500, 502, 503, 504]:
        return base_delay + random.uniform(1, 5)
    else:
        return base_delay


class GeminiImageEdit:
    """Gemini å›¾åƒç¼–è¾‘èŠ‚ç‚¹ - æ”¯æŒå•å›¾å’Œå¤šå›¾è¾“å…¥"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "api_host": ("STRING", {"default": "https://generativelanguage.googleapis.com", "multiline": False}),
                "images": ("IMAGE",),  # æ”¯æŒæ‰¹æ¬¡å›¾åƒ
                "prompt": ("STRING", {"default": "Describe these images and edit them", "multiline": True}),
                "model": (["gemini-2.5-flash-image-preview", "gemini-2.0-flash-preview-image-generation"], {"default": "gemini-2.0-flash-preview-image-generation"}),
                "temperature": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.1}),
                "top_p": ("FLOAT", {"default": 0.95, "min": 0.0, "max": 1.0, "step": 0.05}),
                "max_output_tokens": ("INT", {"default": 8192, "min": 1, "max": 32768}),
                "process_mode": (["first_image_only", "all_images_combined", "each_image_separately"], {"default": "each_image_separately"}),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("edited_image", "response_text")
    FUNCTION = "edit_images"
    CATEGORY = "Gemini"
    
    def edit_images(self, api_key: str, api_host: str, images: torch.Tensor, prompt: str, model: str,
                   temperature: float, top_p: float, max_output_tokens: int, process_mode: str) -> Tuple[torch.Tensor, str]:
        """æ‰¹æ¬¡å¤„ç†å›¾åƒç¼–è¾‘"""
        
        # éªŒè¯APIå¯†é’¥
        if not validate_api_key(api_key):
            raise ValueError("API Keyæ ¼å¼æ— æ•ˆæˆ–ä¸ºç©º")
        
        # éªŒè¯æç¤ºè¯
        if not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")
        
        print(f"ğŸ“Š è¾“å…¥å¼ é‡ä¿¡æ¯: {get_tensor_info(images)}")
        print(f"ğŸ”§ å¤„ç†æ¨¡å¼: {process_mode}")
        
        # è½¬æ¢ä¸ºPILå›¾åƒåˆ—è¡¨
        pil_images = batch_tensor_to_pil_list(images)
        print(f"ğŸ–¼ï¸ è½¬æ¢å¾—åˆ° {len(pil_images)} å¼ å›¾åƒ")
        
        if process_mode == "first_image_only":
            # åªå¤„ç†ç¬¬ä¸€å¼ å›¾åƒ
            return self._process_single_image(api_key, api_host, pil_images[0], prompt, model, temperature, top_p, max_output_tokens)
        
        elif process_mode == "all_images_combined":
            # å°†æ‰€æœ‰å›¾åƒåˆå¹¶å‘é€
            return self._process_combined_images(api_key, api_host, pil_images, prompt, model, temperature, top_p, max_output_tokens)
        
        elif process_mode == "each_image_separately":
            # åˆ†åˆ«å¤„ç†æ¯å¼ å›¾åƒï¼Œè¿”å›æ‰€æœ‰ç»“æœ
            return self._process_images_separately(api_key, api_host, pil_images, prompt, model, temperature, top_p, max_output_tokens)    

    def _process_single_image(self, api_key: str, api_host: str, pil_image: Image.Image, prompt: str, model: str,
                             temperature: float, top_p: float, max_output_tokens: int) -> Tuple[torch.Tensor, str]:
        """å¤„ç†å•å¼ å›¾åƒ"""
        
        # è½¬æ¢ä¸ºbase64
        image_base64 = image_to_base64(pil_image, format='JPEG')
        
        # æ„å»ºAPI URL
        url = f"{api_host}"
        
        # æ„å»ºè¯·æ±‚æ•°æ® - æ›´æ–°ä¸ºåŒ¹é…å®˜æ–¹ç¤ºä¾‹çš„æ ¼å¼
        request_data = {
            "contents": [{
                "parts": [
                    {"text": prompt.strip()},
                    {
                        "inline_data": {
                            "mime_type": "image/jpeg",
                            "data": image_base64
                        }
                    }
                ]
            }],
            "generationConfig": {
                "temperature": temperature,
                "topP": top_p,
                "maxOutputTokens": max_output_tokens
            }
        }
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "Content-Type": "application/json",
            "x-goog-api-key": api_key.strip()
        }
        
        # å‘é€è¯·æ±‚å¹¶å¤„ç†å“åº”
        return self._send_request_and_process(url, headers, request_data, pil_image, model)
    
    def _process_combined_images(self, api_key: str, api_host: str, pil_images: List[Image.Image], prompt: str, model: str,
                                temperature: float, top_p: float, max_output_tokens: int) -> Tuple[torch.Tensor, str]:
        """å¤„ç†å¤šå¼ å›¾åƒï¼ˆåˆå¹¶å‘é€ï¼‰"""
        
        # æ„å»ºåŒ…å«å¤šå¼ å›¾åƒçš„è¯·æ±‚
        parts = [{"text": prompt.strip()}]
        
        # æ·»åŠ æ‰€æœ‰å›¾åƒ
        for i, pil_image in enumerate(pil_images):
            image_base64 = image_to_base64(pil_image, format='JPEG')
            parts.append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": image_base64
                }
            })
            print(f"ğŸ“ æ·»åŠ ç¬¬ {i+1} å¼ å›¾åƒåˆ°è¯·æ±‚ä¸­")
        
        # æ„å»ºAPI URL
        url = f"{api_host}"
        
        # æ„å»ºè¯·æ±‚æ•°æ® - æ›´æ–°ä¸ºåŒ¹é…å®˜æ–¹ç¤ºä¾‹çš„æ ¼å¼
        request_data = {
            "contents": [{
                "parts": parts
            }],
            "generationConfig": {
                "temperature": temperature,
                "topP": top_p,
                "maxOutputTokens": max_output_tokens
            }
        }
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "Content-Type": "application/json",
            "x-goog-api-key": api_key.strip()
        }
        
        # å‘é€è¯·æ±‚å¹¶å¤„ç†å“åº”
        return self._send_request_and_process(url, headers, request_data, pil_images[0], model)
    
    def _process_images_separately(self, api_key: str, api_host: str, pil_images: List[Image.Image], prompt: str, model: str,
                                  temperature: float, top_p: float, max_output_tokens: int) -> Tuple[torch.Tensor, str]:
        """åˆ†åˆ«å¤„ç†æ¯å¼ å›¾åƒ"""
        
        all_edited_images = []
        all_responses = []
        
        for i, pil_image in enumerate(pil_images):
            print(f"ğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(pil_images)} å¼ å›¾åƒ")
            
            # ä¸ºæ¯å¼ å›¾åƒæ·»åŠ åºå·åˆ°æç¤ºè¯ä¸­
            numbered_prompt = f"Image {i+1}/{len(pil_images)}: {prompt}"
            
            # å¤„ç†å•å¼ å›¾åƒ
            edited_image, response = self._process_single_image(
                api_key, api_host, pil_image, numbered_prompt, model,
                temperature, top_p, max_output_tokens
            )
            
            all_edited_images.append(edited_image)
            all_responses.append(f"Image {i+1}/{len(pil_images)}:\n{response}\n")
        
        # åˆå¹¶æ‰€æœ‰ç¼–è¾‘åçš„å›¾åƒå’Œå“åº”
        if len(all_edited_images) > 1:
            combined_images = torch.cat(all_edited_images, dim=0)
        else:
            combined_images = all_edited_images[0]
        
        combined_response = "\n---\n".join(all_responses)
        
        return combined_images, combined_response
    
    def _send_request_and_process(self, url: str, headers: dict, request_data: dict, 
                                 fallback_image: Image.Image, model: str) -> Tuple[torch.Tensor, str]:
        """å‘é€è¯·æ±‚å¹¶å¤„ç†å“åº”"""
        
        max_retries = 5
        timeout = DEFAULT_CONFIG.get("timeout", 120)
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ–¼ï¸ æ­£åœ¨å¤„ç†å›¾åƒ... (å°è¯• {attempt + 1}/{max_retries}) ä½¿ç”¨æ¨¡å‹: {model}")
                
                # å‘é€è¯·æ±‚
                response = requests.post(url, headers=headers, json=request_data, timeout=timeout)
                
                # æˆåŠŸå“åº”
                if response.status_code == 200:
                    result = response.json()
                    print(f"ğŸ“‹ APIå“åº”ç»“æ„: {list(result.keys())}")
                    
                    # æå–æ–‡æœ¬å“åº”å’Œç¼–è¾‘åçš„å›¾ç‰‡
                    response_text = ""
                    edited_image = None
                    
                    if "candidates" in result and result["candidates"]:
                        candidate = result["candidates"][0]
                        if "content" in candidate and "parts" in candidate["content"]:
                            for part in candidate["content"]["parts"]:
                                # æå–æ–‡æœ¬
                                if "text" in part:
                                    response_text += part["text"]
                                
                                # æå–ç¼–è¾‘åçš„å›¾ç‰‡
                                if "inline_data" in part or "inlineData" in part:
                                    inline_data = part.get("inline_data") or part.get("inlineData")
                                    if inline_data and "data" in inline_data:
                                        try:
                                            image_data = inline_data["data"]
                                            image_bytes = base64.b64decode(image_data)
                                            edited_image = Image.open(io.BytesIO(image_bytes))
                                            print("âœ… æˆåŠŸæå–ç¼–è¾‘åçš„å›¾ç‰‡")
                                        except Exception as e:
                                            print(f"âš ï¸ è§£ç å›¾ç‰‡å¤±è´¥: {e}")
                    
                    # å¦‚æœæ²¡æœ‰ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡
                    if edited_image is None:
                        print("âš ï¸ æœªæ£€æµ‹åˆ°ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡")
                        edited_image = fallback_image
                        if not response_text:
                            response_text = "å›¾ç‰‡å¤„ç†è¯·æ±‚å·²å‘é€ï¼Œä½†æœªæ”¶åˆ°ç¼–è¾‘åçš„å›¾ç‰‡"
                    
                    # è½¬æ¢ä¸ºtensor
                    image_tensor = pil_to_tensor(edited_image)
                    
                    print("âœ… å›¾ç‰‡å¤„ç†å®Œæˆ")
                    return (image_tensor, response_text)
                
                # å¤„ç†é”™è¯¯å“åº”
                else:
                    print(f"âŒ HTTPçŠ¶æ€ç : {response.status_code}")
                    try:
                        error_detail = response.json()
                        print(f"âŒ é”™è¯¯è¯¦æƒ…: {json.dumps(error_detail, indent=2, ensure_ascii=False)}")
                    except:
                        print(f"âŒ é”™è¯¯æ–‡æœ¬: {response.text}")
                    
                    if attempt == max_retries - 1:
                        response.raise_for_status()
                    
                    delay = smart_retry_delay(attempt, response.status_code)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except requests.exceptions.RequestException as e:
                error_msg = format_error_message(e)
                print(f"âŒ è¯·æ±‚å¤±è´¥: {error_msg}")
                if attempt == max_retries - 1:
                    raise ValueError(f"APIè¯·æ±‚å¤±è´¥: {error_msg}")
                else:
                    delay = smart_retry_delay(attempt)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except Exception as e:
                error_msg = format_error_message(e)
                print(f"âŒ å¤„ç†å¤±è´¥: {error_msg}")
                raise ValueError(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {error_msg}")


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "GeminiImageEdit": GeminiImageEdit,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GeminiImageEdit": "Gemini å›¾åƒç¼–è¾‘",
}