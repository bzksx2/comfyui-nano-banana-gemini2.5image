"""\nOpenRouter å›¾åƒç¼–è¾‘èŠ‚ç‚¹\næ”¯æŒå•å›¾å’Œå¤šå›¾è¾“å…¥ï¼Œè‡ªåŠ¨å¤„ç†æ‰¹æ¬¡æ•°æ®\n"""

import torch
import numpy as np
from PIL import Image
import io
import base64
import requests
import json
import time
import random
from typing import Optional, Tuple, Dict, Any, List
from openai import OpenAI

try:
    from .tensor_utils import tensor_to_pil, pil_to_tensor, batch_tensor_to_pil_list, get_tensor_info
    from .utils import (
        image_to_base64, base64_to_image,
        validate_api_key, format_error_message, resize_image_for_api
    )
    from .config import DEFAULT_CONFIG
except ImportError:
    from tensor_utils import tensor_to_pil, pil_to_tensor, batch_tensor_to_pil_list, get_tensor_info
    # Fallback utility functions - å¦‚æœæ— æ³•å¯¼å…¥ï¼Œä½¿ç”¨å†…ç½®ç‰ˆæœ¬
    pass
    
    def image_to_base64(image, format='JPEG'):
        buffer = io.BytesIO()
        if format.upper() == 'JPEG' and image.mode in ('RGBA', 'LA', 'P'):
            background = Image.new('RGB', image.size, (255, 255, 255))
            if image.mode == 'P':
                image = image.convert('RGBA')
            background.paste(image, mask=image.split()[-1] if image.mode in ('RGBA', 'LA') else None)
            image = background
        image.save(buffer, format=format, quality=95)
        return base64.b64encode(buffer.getvalue()).decode('utf-8')
    
    def validate_api_key(api_key):
        return api_key and len(api_key.strip()) > 10
    
    def format_error_message(error):
        return str(error)
    
    DEFAULT_CONFIG = {"timeout": 120, "max_retries": 3}


def smart_retry_delay(attempt, error_code=None):
    """æ™ºèƒ½é‡è¯•å»¶è¿Ÿ"""
    base_delay = 2 ** attempt
    
    if error_code == 429:
        rate_limit_delay = 60 + random.uniform(10, 30)
        return max(base_delay, rate_limit_delay)
    elif error_code in [500, 502, 503, 504]:
        return base_delay + random.uniform(1, 5)
    else:
        return base_delay


class OpenRouterImageEdit:
    """OpenRouter å›¾åƒç¼–è¾‘èŠ‚ç‚¹ - æ”¯æŒå•å›¾å’Œå¤šå›¾è¾“å…¥"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "base_url": ("STRING", {"default": "https://openrouter.ai/api/v1", "multiline": False}),
                "site_url": ("STRING", {"default": "https://your-site.com", "multiline": False}),
                "site_name": ("STRING", {"default": "Your Site Name", "multiline": False}),
                "images": ("IMAGE",),  # æ”¯æŒæ‰¹æ¬¡å›¾åƒ
                "prompt": ("STRING", {"default": "Describe these images and edit them", "multiline": True}),
                "model": ([
                    "google/gemini-2.5-flash-image-preview:free",
                    "google/gemini-2.5-flash-image-preview"
                ], {"default": "google/gemini-2.5-flash-image-preview:free"}),
                "temperature": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.1}),
                "top_p": ("FLOAT", {"default": 0.95, "min": 0.0, "max": 1.0, "step": 0.05}),
                "max_tokens": ("INT", {"default": 8192, "min": 1, "max": 32768}),
                "process_mode": ([
                    "first_image_only",
                    "all_images_combined",
                    "each_image_separately"
                ], {"default": "each_image_separately"}),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("edited_image", "response_text")
    FUNCTION = "edit_images"
    CATEGORY = "OpenRouter"
    
    def edit_images(self, api_key: str, base_url: str, site_url: str, site_name: str, images: torch.Tensor,
                    prompt: str, model: str, temperature: float, top_p: float,
                    max_tokens: int, process_mode: str) -> Tuple[torch.Tensor, str]:
        """æ‰¹æ¬¡å¤„ç†å›¾åƒç¼–è¾‘"""
        
        # éªŒè¯APIå¯†é’¥
        if not validate_api_key(api_key):
            raise ValueError("API Keyæ ¼å¼æ— æ•ˆæˆ–ä¸ºç©º")
        
        # éªŒè¯æç¤ºè¯
        if not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")
        
        print(f"ğŸ“Š è¾“å…¥å¼ é‡ä¿¡æ¯: {get_tensor_info(images)}")
        print(f"ğŸ”§ å¤„ç†æ¨¡å¼: {process_mode}")
        
        # è½¬æ¢ä¸ºPILå›¾åƒåˆ—è¡¨
        pil_images = batch_tensor_to_pil_list(images)
        print(f"ğŸ–¼ï¸ è½¬æ¢å¾—åˆ° {len(pil_images)} å¼ å›¾åƒ")
        
        if process_mode == "first_image_only":
            # åªå¤„ç†ç¬¬ä¸€å¼ å›¾åƒ
            return self._process_single_image(
                api_key, base_url, site_url, site_name, pil_images[0],
                prompt, model, temperature, top_p, max_tokens
            )
        
        elif process_mode == "all_images_combined":
            # å°†æ‰€æœ‰å›¾åƒåˆå¹¶å‘é€
            return self._process_combined_images(
                api_key, base_url, site_url, site_name, pil_images,
                prompt, model, temperature, top_p, max_tokens
            )
        
        elif process_mode == "each_image_separately":
            # åˆ†åˆ«å¤„ç†æ¯å¼ å›¾åƒï¼Œè¿”å›æ‰€æœ‰ç»“æœ
            return self._process_images_separately(
                api_key, base_url, site_url, site_name, pil_images,
                prompt, model, temperature, top_p, max_tokens
            )

    def _process_single_image(self, api_key: str, base_url: str, site_url: str, site_name: str,
                             pil_image: Image.Image, prompt: str, model: str,
                             temperature: float, top_p: float,
                             max_tokens: int) -> Tuple[torch.Tensor, str]:
        """å¤„ç†å•å¼ å›¾åƒ"""
        
        # è½¬æ¢ä¸ºbase64
        image_base64 = image_to_base64(pil_image, format='JPEG')
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        client = OpenAI(
            base_url=base_url,
            api_key=api_key.strip()
        )
        
        try:
            # åˆ›å»ºèŠå¤©å®Œæˆè¯·æ±‚
            completion = client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": site_url,
                    "X-Title": site_name,
                },
                model=model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt.strip()
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}"
                                }
                            }
                        ]
                    }
                ],
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens
            )
            
            # æå–å“åº”æ–‡æœ¬
            response_text = completion.choices[0].message.content
            
            # ç”±äºOpenRouterç›®å‰ä¸æ”¯æŒå›¾åƒç¼–è¾‘ï¼Œè¿”å›åŸå›¾
            image_tensor = pil_to_tensor(pil_image)
            
            print("âœ… å›¾ç‰‡å¤„ç†å®Œæˆ")
            return (image_tensor, response_text)
            
        except Exception as e:
            error_msg = format_error_message(e)
            print(f"âŒ å¤„ç†å¤±è´¥: {error_msg}")
            raise ValueError(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {error_msg}")
    
    def _process_combined_images(self, api_key: str, base_url: str, site_url: str, site_name: str,
                                pil_images: List[Image.Image], prompt: str, model: str,
                                temperature: float, top_p: float,
                                max_tokens: int) -> Tuple[torch.Tensor, str]:
        """å¤„ç†å¤šå¼ å›¾åƒï¼ˆåˆå¹¶å‘é€ï¼‰"""
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        client = OpenAI(
            base_url=base_url,
            api_key=api_key.strip()
        )
        
        try:
            # æ„å»ºæ¶ˆæ¯å†…å®¹
            content = [
                {
                    "type": "text",
                    "text": prompt.strip()
                }
            ]
            
            # æ·»åŠ æ‰€æœ‰å›¾åƒ
            for i, pil_image in enumerate(pil_images):
                image_base64 = image_to_base64(pil_image, format='JPEG')
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{image_base64}"
                    }
                })
                print(f"ğŸ“ æ·»åŠ ç¬¬ {i+1} å¼ å›¾åƒåˆ°è¯·æ±‚ä¸­")
            
            # åˆ›å»ºèŠå¤©å®Œæˆè¯·æ±‚
            completion = client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": site_url,
                    "X-Title": site_name,
                },
                model=model,
                messages=[{
                    "role": "user",
                    "content": content
                }],
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens
            )
            
            # æå–å“åº”æ–‡æœ¬
            response_text = completion.choices[0].message.content
            
            # ç”±äºOpenRouterç›®å‰ä¸æ”¯æŒå›¾åƒç¼–è¾‘ï¼Œè¿”å›ç¬¬ä¸€å¼ å›¾
            image_tensor = pil_to_tensor(pil_images[0])
            
            print("âœ… å›¾ç‰‡å¤„ç†å®Œæˆ")
            return (image_tensor, response_text)
            
        except Exception as e:
            error_msg = format_error_message(e)
            print(f"âŒ å¤„ç†å¤±è´¥: {error_msg}")
            raise ValueError(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {error_msg}")
    
    def _process_images_separately(self, api_key: str, base_url: str, site_url: str, site_name: str,
                                  pil_images: List[Image.Image], prompt: str, model: str,
                                  temperature: float, top_p: float,
                                  max_tokens: int) -> Tuple[torch.Tensor, str]:
        """åˆ†åˆ«å¤„ç†æ¯å¼ å›¾åƒ"""
        
        all_edited_images = []
        all_responses = []
        
        for i, pil_image in enumerate(pil_images):
            print(f"ğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(pil_images)} å¼ å›¾åƒ")
            
            # ä¸ºæ¯å¼ å›¾åƒæ·»åŠ åºå·åˆ°æç¤ºè¯ä¸­
            numbered_prompt = f"Image {i+1}/{len(pil_images)}: {prompt}"
            
            # å¤„ç†å•å¼ å›¾åƒ
            edited_image, response = self._process_single_image(
                api_key, base_url, site_url, site_name, pil_image, numbered_prompt,
                model, temperature, top_p, max_tokens
            )
            
            all_edited_images.append(edited_image)
            all_responses.append(f"Image {i+1}/{len(pil_images)}:\n{response}\n")
        
        # åˆå¹¶æ‰€æœ‰ç¼–è¾‘åçš„å›¾åƒå’Œå“åº”
        if len(all_edited_images) > 1:
            combined_images = torch.cat(all_edited_images, dim=0)
        else:
            combined_images = all_edited_images[0]
        
        combined_response = "\n---\n".join(all_responses)
        
        return combined_images, combined_response


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "OpenRouterImageEdit": OpenRouterImageEdit,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "OpenRouterImageEdit": "OpenRouter å›¾åƒç¼–è¾‘",
}